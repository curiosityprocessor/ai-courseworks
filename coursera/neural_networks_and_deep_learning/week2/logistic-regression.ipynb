{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cb029f8",
   "metadata": {},
   "source": [
    "# Overview\n",
    "## Logistic regression with a neural network mindset\n",
    "Build general architecture of a learning algorithm:\n",
    "- init parameters\n",
    "- calculate the cost function and its gradient\n",
    "- use optimization algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987d6b3c",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "Given a dataset:\n",
    "- a training set of `m_train` images labeled as cat `(y=1)` or non-cat `(y=0)`\n",
    "- a test set of `m_test` images labeled as cat or non-cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894c152",
   "metadata": {},
   "source": [
    "## 1 Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141d6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09500056",
   "metadata": {},
   "source": [
    "# 2 Pre-processing\n",
    "In order to pre-process images of shape (num_px, num_px, 3) into numpy-array of shape (num_px * num_px * 3, 1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185434ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 20\n",
      "number of test examples: 10\n",
      "train_set_x_orig shape: (20, 64, 64, 3)\n",
      "train_set_x_flatten shape: (12288, 20)\n",
      "test_set_x_orig shape: (10, 64, 64, 3)\n",
      "test_set_x_flatten shape: (12288, 10)\n"
     ]
    }
   ],
   "source": [
    "# Given a dataset\n",
    "# train_set_x_orig.shape: (m_train, num_px, num_px, 3)\n",
    "train_set_x_orig = np.random.randint(0, 256, size = (20, 64, 64, 3))\n",
    "# test_set_x_orig.shape: (m_test, num_px, num_px, 3)\n",
    "test_set_x_orig = np.random.randint(0, 256, size = (10, 64, 64, 3))\n",
    "m_train = train_set_x_orig.shape[0] # number of training examples\n",
    "m_test = test_set_x_orig.shape[0] # number of test examples\n",
    "train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T # reshape to (num_px * num_px * 3, m_train)\n",
    "test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T # reshape to (num_px * num_px * 3, m_test)\n",
    "\n",
    "print(\"number of training examples: \" + str(m_train))\n",
    "print(\"number of test examples: \" + str(m_test))\n",
    "print(\"train_set_x_orig shape: \" + str(train_set_x_orig.shape))\n",
    "print(\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print(\"test_set_x_orig shape: \" + str(test_set_x_orig.shape))\n",
    "print(\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f510f12f",
   "metadata": {},
   "source": [
    "Assuming pixels refers to RGB values with range from 0 to 255 (inclusive),  \n",
    "standardize the dataset by simple division of row by 255 (max value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8b724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802abcf5",
   "metadata": {},
   "source": [
    "# 3 Architecture for learning algorithm\n",
    "Mathematical expression  \n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6818f731",
   "metadata": {},
   "source": [
    "# 4 Building the algorithm\n",
    "1. Define model structure\n",
    "2. Initialize model parameters\n",
    "3. Loop\n",
    "- Calculate current loss (forward propagation)\n",
    "- Calculate current gradient (backward propagation)\n",
    "- Update parameters (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6bb653",
   "metadata": {},
   "source": [
    "## 4.1 Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86694f9c",
   "metadata": {},
   "source": [
    "### Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efccea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / ( 1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbca89",
   "metadata": {},
   "source": [
    "## 4.2 Initializing parameters\n",
    "initialize `w` as parameter of zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a207372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: [[0.]\n",
      " [0.]] shape: (2, 1)\n",
      "b: 0.0 type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    w = np.zeros(shape=(dim, 1))\n",
    "    b = 0.0\n",
    "    return w, b\n",
    "\n",
    "dim = 2\n",
    "w, b = initialize_with_zeros(dim)\n",
    "print(\"w: \" + str(w) + \" shape: \" + str(w.shape))\n",
    "print(\"b: \" + str(b) + \" type: \" + str(type(b)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ec99f",
   "metadata": {},
   "source": [
    "## 4.3 Forward and back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06cb61",
   "metadata": {},
   "source": [
    "### propagate\n",
    "implement a propagate function that computes the cost function and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6928491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[ 0.25071532]\n",
      " [-0.06604096]]\n",
      "db = -0.1250040450043965\n",
      "cost = 0.15900537707692405\n"
     ]
    }
   ],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "\n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y)\n",
    "\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    grds = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grds, cost\n",
    "\n",
    "w =  np.array([[1.], [2]])\n",
    "b = 1.5\n",
    "\n",
    "# X is using 3 examples, with 2 features each\n",
    "# Each example is stacked column-wise\n",
    "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
    "Y = np.array([[1, 1, 0]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "assert type(grads[\"dw\"]) == np.ndarray\n",
    "assert grads[\"dw\"].shape == (2, 1)\n",
    "assert type(grads[\"db\"]) == np.float64\n",
    "\n",
    "\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b931c9a",
   "metadata": {},
   "source": [
    "## 4.4 Optimization\n",
    "- after initializing parameters\n",
    "- and computing the cost function and its gradient\n",
    "- now update the parameters using gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c8986",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "implmenet the optimization function to learn `w` and `b` by minimizing cost function `J`.  \n",
    "For parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21b80df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.80956046]\n",
      " [2.0508202 ]]\n",
      "b = 1.5948713189708588\n",
      "dw = [[ 0.17860505]\n",
      " [-0.04840656]]\n",
      "db = -0.08888460336847771\n",
      "Costs = [array(0.15900538)]\n"
     ]
    }
   ],
   "source": [
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grds, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        dw = grds[\"dw\"]\n",
    "        db = grds[\"db\"]\n",
    "\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "            if print_cost:\n",
    "                print(\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    grds = {\"dw\": dw, \"db\": db}\n",
    "    return params, grds, costs\n",
    "\n",
    "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print(\"Costs = \" + str(costs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879981cb",
   "metadata": {},
   "source": [
    "### Predict\n",
    "Using the previous function's output of the learned `w` and `b`, predict the labels for dataset X.  \n",
    "Implement predict function:\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21ef3cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "\n",
    "    for i in range(A.shape[1]):\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        else:\n",
    "            Y_prediction[0,i] = 0\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "w = np.array([[0.1124579], [0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a9049c",
   "metadata": {},
   "source": [
    "# 5 Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87cc241",
   "metadata": {},
   "source": [
    "### Model\n",
    "Implement the model function:\n",
    "- Y_prediction_test for your predictions on the test set\n",
    "- Y_prediction_train for your predictions on the train set\n",
    "- parameters, grads, costs for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a74bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {}%\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {}%\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "    \n",
    "    d = {\n",
    "        \"costs\" : costs, \n",
    "        \"Y_prediction_test\" : Y_prediction_test, \n",
    "        \"Y_prediction_train\" : Y_prediction_train, \n",
    "        \"w\" : w, \n",
    "        \"b\" : b, \n",
    "        \"learning_rate\" : learning_rate, \n",
    "        \"num_iterations\" : num_iterations\n",
    "    }\n",
    "\n",
    "    return d\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
